{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressor(HybridBlock):\n",
    "    def __init__(self, in_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        with self.name_scope():\n",
    "            self.w = self.params.get(\n",
    "                'w', shape=(1, in_units)\n",
    "            )\n",
    "\n",
    "            self.b = self.params.get(\n",
    "                'b', shape=(1, 1)\n",
    "            )\n",
    "\n",
    "    def hybrid_forward(self, F, X, w, b):\n",
    "        # Change shape of b to comply with MXnet addition API\n",
    "        b = F.broadcast_axis(b, axis=(0,1), size=(34, 1))\n",
    "        y = F.dot(X, w, transpose_b=True) + b\n",
    "\n",
    "        return F.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(A, X):\n",
    "    hidden_layer_specs = [(4, 'tanh'), (2, 'tanh')] # Format: (units in layer, activation function)\n",
    "    in_units = in_units=X.shape[1]\n",
    "  \n",
    "    features = HybridSequential()\n",
    "    with features.name_scope():\n",
    "        for i, (layer_size, activation_func) in enumerate(hidden_layer_specs):\n",
    "            layer = SpectralRule(\n",
    "                A, in_units=in_units, out_units=layer_size, \n",
    "                activation=activation_func)\n",
    "            features.add(layer)\n",
    "\n",
    "            in_units = layer_size\n",
    "    return features, in_units\n",
    "\n",
    "\n",
    "def build_model(A, X):\n",
    "    model = HybridSequential()\n",
    "    hidden_layer_specs = [(4, 'tanh'), (2, 'tanh')]\n",
    "    in_units = in_units=X.shape[1]\n",
    "\n",
    "    with model.name_scope():\n",
    "        features, out_units = build_features(A, X)\n",
    "        model.add(features)\n",
    "\n",
    "        classifier = LogisticRegressor(out_units)\n",
    "        model.add(classifier)\n",
    "\n",
    "    model.hybridize()\n",
    "    model.initialize(Uniform(1))\n",
    "\n",
    "    return model, features\n",
    "\n",
    "def train(model, features, X, X_train, y_train, epochs):\n",
    "    cross_entropy = SigmoidBinaryCrossEntropyLoss(from_sigmoid=True)\n",
    "    trainer = Trainer(model.collect_params(), 'sgd', {'learning_rate': 0.001, 'momentum': 1})\n",
    "\n",
    "    feature_representations = [features(X).asnumpy()]\n",
    "\n",
    "    for e in range(1, epochs + 1):\n",
    "        cum_loss = 0\n",
    "        cum_preds = []\n",
    "\n",
    "        for i, x in enumerate(X_train):\n",
    "            y = array(y_train)[i]\n",
    "            with autograd.record():\n",
    "                preds = model(X)[x]\n",
    "                loss = cross_entropy(preds, y)\n",
    "            loss.backward()\n",
    "            trainer.step(1)\n",
    "\n",
    "            cum_loss += loss.asscalar()\n",
    "            cum_preds += [preds.asscalar()]\n",
    "\n",
    "        feature_representations.append(features(X).asnumpy())\n",
    "            \n",
    "        if (e % (epochs//10)) == 0:\n",
    "            print(f\"Epoch {e}/{epochs} -- Loss: {cum_loss: .4f}\")\n",
    "            print(cum_preds)\n",
    "    return feature_representations\n",
    "\n",
    "def predict(model, X, nodes):\n",
    "    preds = model(X)[nodes].asnumpy().flatten()\n",
    "    return np.where(preds >= 0.5, 1, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
