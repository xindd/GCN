{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\wx\\2019上课题\\程序\\GCN\\gcn\\loadNetParams.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.res['entryID'] = self.res[1].map(geneset2entryID).apply(lambda x: 'entry' + str(x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ppi net data...\n",
      "loading relation between protein and entry ...\n",
      "loading entry net...\n",
      "loading relation between entry and pathway...\n",
      "loading pathway net...\n",
      "build net models...\n",
      "initialize net...\n",
      "loading expression data...\n",
      "prepare train samples...\n",
      "training...\n",
      "epoch 1, loss 3.2498, train acc 0.102,  time 147128.9 sec\n"
     ]
    }
   ],
   "source": [
    "# %load train.py\n",
    "#!/usr/bin/python3\n",
    "# @Author: XinWang\n",
    "# @Time:2019/7/24 9:59\n",
    "# @FileName: train.py\n",
    "# @Software: PyCharm\n",
    "\n",
    "from loadNetParams import *\n",
    "from loadData import load_all_data\n",
    "from models import *\n",
    "\n",
    "from mxnet import gluon, init\n",
    "from mxnet import autograd\n",
    "import mxboard as mb\n",
    "import time\n",
    "\n",
    "\n",
    "def evaluate_accuracy(data_iter, net):\n",
    "    acc, n = 0.0, 0\n",
    "    for X, Y in data_iter:\n",
    "        pre = net(X)\n",
    "        acc += (pre.argmax(axis=1) == Y.argmax(axis=1)).sum().asscalar()\n",
    "        n += Y.shape[0]\n",
    "        if n == 20:\n",
    "            break\n",
    "    return acc / n\n",
    "\n",
    "\n",
    "def train(data_iter, net, cross_entropy, trainer, num_epochs, batch_size):\n",
    "    sw = mb.SummaryWriter(logdir='./logs', flush_secs=2)\n",
    "    params = net.collect_params('.*W|.*dense')\n",
    "    param_names = params.keys()\n",
    "    ls = 0\n",
    "    # train_x, train_y, test_x, test_y = allData['train_x'], allData['train_y'], allData['test_x'], allData['test_y']\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss_sum, train_acc_sum, n, start = 0., 0., 0., time.time()\n",
    "        for X, Y in data_iter:\n",
    "            # X.attach_grad()\n",
    "            with autograd.record():\n",
    "                pre = net(X.reshape(*X.shape, 1))\n",
    "                loss = cross_entropy(pre, Y).sum()\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "\n",
    "            # 记录\n",
    "            train_loss_sum += loss.asscalar()\n",
    "            train_acc_sum += (pre.argmax(axis=1) == Y).sum().asscalar()\n",
    "            n += len(Y)\n",
    "            sw.add_histogram(tag='cross_entropy', values=train_loss_sum / n, global_step=ls)\n",
    "\n",
    "            for i, name in enumerate(param_names):\n",
    "                sw.add_histogram(tag=name,\n",
    "                                 values=net.collect_params()[name].grad(),\n",
    "                                 global_step=ls, bins=1000)\n",
    "            ls += 1\n",
    "        # test_acc = evaluate_accuracy(test_x, test_y, net)\n",
    "        print('epoch %d, loss %.4f, train acc %.3f,  time %.1f sec' %\n",
    "              (epoch + 1, train_loss_sum / n, train_acc_sum / n, time.time() - start))\n",
    "    sw.close()\n",
    "    return net\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = LoadNetParams()\n",
    "    net, feature = train_model(dataset)\n",
    "    net.initialize()\n",
    "    print('initialize net...')\n",
    "    batch_size = 10 \n",
    "    print('loading expression data...')\n",
    "    exp = pd.read_csv('data/exp_data.csv')\n",
    "    print('prepare train samples...')\n",
    "    train_samples, validation_samples, test_samples = load_all_data()\n",
    "    train_x = nd.array(exp[train_samples['sampleID']]).transpose()\n",
    "    train_y = nd.array(train_samples['label'])\n",
    "\n",
    "    dataset = gluon.data.ArrayDataset(train_x, train_y)\n",
    "    data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True)\n",
    "\n",
    "    cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "    trainer = gluon.trainer.Trainer(net.collect_params(), 'adam', {'learning_rate': 0.0001})\n",
    "    print('training...')\n",
    "    train(data_iter, net, cross_entropy, trainer, 5, batch_size)\n",
    "    print('save params...')\n",
    "    net.save_parameters('data/net_params_win')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = nd.array([[1,2,3],[3,4,5]])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
